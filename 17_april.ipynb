{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a2681d-e297-47ad-87f5-b46766616cb8",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a22a44-5fc4-4a4c-9d22-a72faad8ba6d",
   "metadata": {},
   "source": [
    "A Gradient boosting, an ensemble machine learning technique, constructs a robust predictive model by sequentially combining multiple weak models. It minimizes errors using a gradient descent-like approach during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d577dc-9361-49ec-8dc9-8c03f5e3b4e1",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27e990-8609-4c09-aa75-4392324e2a56",
   "metadata": {},
   "source": [
    "Boosting\" in machine learning is a way of combining multiple simple models into a single composite model. This is also why boosting is known as an additive model, since simple models (also known as weak learners) are added one at a time, while keeping existing trees in the model unchanged. As we combine more and more simple models, the complete final model becomes a stronger predictor. The term \"gradient\" in \"gradient boosting\" comes from the fact that the algorithm uses gradient descent to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22564af-91e5-4370-977e-1eea5e38a551",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b7041-91cb-43f5-9d47-c0412b7c03ea",
   "metadata": {},
   "source": [
    "In this notebook, we will explore two methods for hyperparameter tuning a machine learning model. In contrast to model parameters which are learned during training, model hyperparameters are set by the data scientist ahead of training and control implementation aspects of the model. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will not be the best across all datasets. The process of hyperparameter tuning (also called hyperparameter optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset \n",
    "\n",
    "\n",
    "There are several approaches to hyperparameter tuning:-\n",
    "\n",
    "Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "Grid Search: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!\n",
    "\n",
    "Random search: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.\n",
    "\n",
    "Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd80f5-bb07-4bda-9d89-0a4685fd9cad",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4cb5c-a413-4fd4-b5c4-0dfa5d91b97d",
   "metadata": {},
   "source": [
    "Weak learners are models that perform slightly better than random guessing. Strong learners are models that have arbitrarily good accuracy. Weak and strong learners are tools from computational learning theory and provide the basis for the development of the boosting class of ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd342544-7136-45b9-9526-4c6b9b92cf62",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f46df-d6ef-4f32-9e16-18a236db673b",
   "metadata": {},
   "source": [
    "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b84467-bb1a-4428-bd15-6ad3275636a8",
   "metadata": {},
   "source": [
    "The gradient-boosting regressor works by iteratively building an ensemble of weak learners, where each subsequent weak learner is trained to correct the mistakes made by the previous ones. The predictions from all weak learners are combined to make the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8294e-61ed-4fb1-a371-0a80488944d1",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e14ea-2057-4a23-92bb-e8213c380a2d",
   "metadata": {},
   "source": [
    "Since we are doing regression, we will use MSE:-\n",
    "Step 1: Make an initial prediction. Gradient boosting is an algorithm that gradually increases its accuracy. ...\n",
    "Step 2: Calculate the pseudo-residuals. \n",
    "Step 3: Build a weak learner. \n",
    "Step 4: Iterate.\n",
    "\n",
    "\n",
    "Constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm:-\n",
    "\n",
    "Gradient boosting is the best: its accuracy and performance are unmatched for tabular supervised learning tasks.\n",
    "Gradient boosting is highly versatile: it can be used in many important tasks such as regression, classification, ranking, and survival analysis.\n",
    "Gradient boosting is interpretable: unlike black-box algorithms like neural networks, gradient boosting does not sacrifice interpretability for performance. It works like a Swiss watch and yet, with patience, you can teach how it works to a school kid.\n",
    "Gradient boosting is well-implemented: it is not one of those algorithms that have little practical value. Various gradient boosting libraries like XGBoost and LightGBM in Python are used by hundreds of thousands of people.\n",
    "Gradient boosting wins: since 2015, professionals have used it to consistently win tabular competitions on platforms like Kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
